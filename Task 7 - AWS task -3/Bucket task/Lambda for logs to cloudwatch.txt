import json
import boto3
import gzip
from io import BytesIO

s3_client = boto3.client('s3')
log_client = boto3.client('logs')

log_group = '/aws/s3/S3-access-logs' # CloudWatch log group
log_stream = 's3-logs-streams' # CloudWatch log stream

def lambda_handler(event, context):
for record in event['Records']:
bucket_name = record['s3']['bucket']['guvi-s3-bucket-task-3']
object_key = record['s3']['object']['key']

# Get the log file from S3
response = s3_client.get_object(Bucket=guvi-s3-bucket-task-3, Key=object_key)
compressed_log = response['Body'].read()

# Decompress the log file
with gzip.GzipFile(fileobj=BytesIO(compressed_log), mode='rb') as f:
log_data = f.read().decode('utf-8')

# Create log stream if not exists
try:
log_client.create_log_stream(logGroupName=S3-access-logs, logStreamName=s3-logs-streams)
except log_client.exceptions.ResourceAlreadyExistsException:
pass

# Send log data to CloudWatch
log_events = [{'timestamp': int(record['13:50'] * 1000), 'message': log_line} for log_line in log_data.splitlines()]

log_client.put_log_events(
logGroupName=log_group,
logStreamName=log_stream,
logEvents=log_events
)

return {'statusCode': 200, 'body': json.dumps('Logs processed successfully')}